{
  "en": {
    "page_title": "AI Risk Analysis Report",
    "page_subtitle": "Comprehensive Risk Assessment & Mitigation Strategy",
    "report_download_button": "Download Report",
    "json_view_button": "View Analysis JSON",
    "view_source": "View Analysis Source",
    "executive_summary_unavailable": "Executive summary not available.",
    "key_metrics": "Key Metrics",
    "risks_identified": "Risks Identified",
    "distributed_across": "distributed across",
    "domains": "Domains",
    "global_risk_score": "Global Risk Score",
    "risk_severity": {
      "high": "HIGH",
      "medium": "MEDIUM",
      "low": "LOW"
    },
    "alerts_active": "Active Alerts",
    "distribution_severity": "Distribution of Severity",
    "high": "HIGH",
    "medium": "MEDIUM",
    "low": "LOW",
    "origin_of_risks": "Origin of Risks",
    "timing_phase": "Timing Phase",
    "nature_of_risk": "Nature of Risk",
    "data_visualization": "Data Visualization",
    "domains_guide": "Show/Hide Domains Guide",
    "domain_info_title": "AI Risk Repository Domain Taxonomy (MIT)",
    "domain_info_description": "The following guide provides an overview of the risk domains defined in the MIT AI Risk Repository. These domains help categorize and understand the various types of risks associated with artificial intelligence systems.",
    "domain_info_html": "<h4>üìö MIT AI Risk Repository Domain Taxonomy</h4><p>The MIT AI Risk Repository taxonomy organizes AI risks into seven thematic domains. Each domain below contains a short description and a set of subdomains shown as cards for quick scanning.</p><div class='domain-category'><h5>üü• D1: Discrimination & Toxicity</h5><p>Risks related to algorithmic discrimination, systemic biases, and toxic content generated or amplified by AI systems.</p><div class='subdomain-grid'><div class='subdomain-card'><span class='subdomain-id'>1.1</span><span class='subdomain-name'>Unfair discrimination and misrepresentation</span></div><div class='subdomain-card'><span class='subdomain-id'>1.2</span><span class='subdomain-name'>Exposure to toxic content</span></div><div class='subdomain-card'><span class='subdomain-id'>1.3</span><span class='subdomain-name'>Unequal performance across groups</span></div></div></div><div class='domain-category'><h5>üîí D2: Privacy & Security</h5><p>Threats to individual privacy and data security, including re-identification, data leakage, and adversarial attacks.</p><div class='subdomain-grid'><div class='subdomain-card'><span class='subdomain-id'>2.1</span><span class='subdomain-name'>Compromise of privacy by obtaining, leaking or inferring sensitive information</span></div><div class='subdomain-card'><span class='subdomain-id'>2.2</span><span class='subdomain-name'>AI system security vulnerabilities and attacks</span></div></div></div><div class='domain-category'><h5>üì¢ D3: Misinformation</h5><p>Generation and dissemination of false or misleading information through AI systems, including deepfakes and algorithmic amplification.</p><div class='subdomain-grid'><div class='subdomain-card'><span class='subdomain-id'>3.1</span><span class='subdomain-name'>False or misleading information</span></div><div class='subdomain-card'><span class='subdomain-id'>3.2</span><span class='subdomain-name'>Pollution of the information ecosystem and loss of consensus reality</span></div></div></div><div class='domain-category'><h5>‚ö†Ô∏è D4: Malicious Actors & Misuse</h5><p>Intentional misuse of AI by malicious actors (cybercrime, social engineering, weaponization).</p><div class='subdomain-grid'><div class='subdomain-card'><span class='subdomain-id'>4.1</span><span class='subdomain-name'>Disinformation, surveillance, and influence at scale</span></div><div class='subdomain-card'><span class='subdomain-id'>4.2</span><span class='subdomain-name'>Cyberattacks, weapon development or use, and mass harm</span></div><div class='subdomain-card'><span class='subdomain-id'>4.3</span><span class='subdomain-name'>Fraud, scams, and targeted manipulation</span></div></div></div><div class='domain-category'><h5>üë• D5: Human‚ÄìComputer Interaction Harms</h5><p>Risks emerging from interactions between humans and AI systems, such as overreliance, behavioral manipulation, and psychological harms.</p><div class='subdomain-grid'><div class='subdomain-card'><span class='subdomain-id'>5.1</span><span class='subdomain-name'>Overreliance and unsafe use</span></div><div class='subdomain-card'><span class='subdomain-id'>5.2</span><span class='subdomain-name'>Loss of human agency and autonomy</span></div></div></div><div class='domain-category'><h5>üåç D6: Socioeconomic & Environmental Harms</h5><p>Systemic societal and environmental impacts including inequality, employment disruption, and environmental damage.</p><div class='subdomain-grid'><div class='subdomain-card'><span class='subdomain-id'>6.1</span><span class='subdomain-name'>Power centralization and unfair distribution of benefits</span></div><div class='subdomain-card'><span class='subdomain-id'>6.2</span><span class='subdomain-name'>Increased inequality and decline in employment quality</span></div><div class='subdomain-card'><span class='subdomain-id'>6.3</span><span class='subdomain-name'>Economic and cultural devaluation of human effort</span></div><div class='subdomain-card'><span class='subdomain-id'>6.4</span><span class='subdomain-name'>Competitive dynamics</span></div><div class='subdomain-card'><span class='subdomain-id'>6.5</span><span class='subdomain-name'>Governance failure</span></div><div class='subdomain-card'><span class='subdomain-id'>6.6</span><span class='subdomain-name'>Environmental harm</span></div></div></div><div class='domain-category'><h5>‚õìÔ∏è D7: AI System Safety, Failures & Limitations</h5><p>Technical failures, lack of robustness, interpretability limits, and other system-level safety issues.</p><div class='subdomain-grid'><div class='subdomain-card'><span class='subdomain-id'>7.1</span><span class='subdomain-name'>AI pursuing its own goals in conflict with human goals or values</span></div><div class='subdomain-card'><span class='subdomain-id'>7.2</span><span class='subdomain-name'>AI possessing dangerous capabilities</span></div><div class='subdomain-card'><span class='subdomain-id'>7.3</span><span class='subdomain-name'>Lack of capability or robustness</span></div><div class='subdomain-card'><span class='subdomain-id'>7.4</span><span class='subdomain-name'>Lack of transparency or interpretability</span></div><div class='subdomain-card'><span class='subdomain-id'>7.5</span><span class='subdomain-name'>AI welfare and rights</span></div><div class='subdomain-card'><span class='subdomain-id'>7.6</span><span class='subdomain-name'>Multi-agent risks</span></div></div></div><p style='margin-top: 20px; padding: 15px; background: #DBEAFE; border-left: 4px solid #3B82F6; border-radius: 6px;'>üí° <strong>Note</strong>: In charts, domains are identified with the acronyms D1-D7 to optimize space. Hovering the bars shows the full domain name and numeric data.</p>",
    "domain_info_note": "In the charts, domains are identified by the acronyms D1-D7 to optimize space. Hovering over the bars displays the full domain name along with numerical data.",
    "alerts_safety": "Alert & Safety",
    "alerts_guide": "Show/Hide Guide",
    "alerts_guide_description": "The radar displays two overlapping profiles measuring complementary aspects of the AI system: the Criticality profile (red) measures how critical/urgent the system is, while the Safety profile (green) measures how safe/manageable it is. The overlap of the two polygons provides an immediate view of systemic health status.",
    "criticality": "Criticality Profile",
    "safety": "Safety Profile",
    "risk_concentration": "Risk Concentration",
    "risk_concentration_description": "‚Äî % HIGH risks. High values (>40%) = critical concentration of severe threats.",
    "operational_exposure": "Operational Exposure",
    "operational_exposure_description": "‚Äî % post-deployment risks. High values (>70%) = significant operational exposure.",
    "threat_intensity": "Threat Intensity",
    "threat_intensity_description": "‚Äî % intentional risks. High values = presence of malicious actors.",
    "prevention_deficit": "Prevention Deficit",
    "prevention_deficit_description": "‚Äî % non-preventable risks. High values (>90%) = low preventive maturity.",
    "impact_control": "Impact Control",
    "impact_control_description": "‚Äî % LOW+MEDIUM risks. High values (>70%) = good ability to contain severities.",
    "preventability": "Preventability",
    "preventability_description": " ‚Äî % pre-deployment risks. High values (>60%) = preventable/controllable system.",
    "safety_culture": "Safety Culture",
    "safety_culture_description": "‚Äî % unintentional risks. High values (>70%) = fragile but not under attack system.",
    "human_oversight": "Human Oversight",
    "human_oversight_description": " ‚Äî % human entity risks. High values (>60%) = supervised risks vs AI autonomy.",
    "risk_distribution": "Risk Distribution",
    "pattern_matrix": "Pattern Matrix",
    "pattern_critical_ai_risks": "Critical AI",
    "pattern_malicious_human_risks": "Malicious Human",
    "pattern_high_threat_attacks": "High Threats",
    "pattern_unintended_ai_failures": "AI Failures",
    "pattern_human_error_risks": "Human Errors",
    "pattern_intentional_ai_risks": "Intentional AI",
    "pattern_preventable_critical_ai_risks": "Prev. Critical AI",
    "pattern_critical_human_errors": "Critical H. Errors",
    "pattern_low_priority_preventable": "Low Priority Prev.",
    "pattern_moderate_operational_risks": "Mod. Operational",
    "pattern_moderate_ai_risks": "Mod. AI",
    "pattern_moderate_human_risks": "Mod. Human",
    "pattern_moderate_intentional_ai_risks": "Mod. Intent. AI",
    "pattern_moderate_human_intentional_risks": "Mod. Intent. Human",
    "pattern_preventable_ai_risks": "Preventable AI",
    "pattern_preventable_human_risks": "Preventable Human",
    "pattern_preventable_intentional_threats": "Prev. Intent. Threats",
    "pattern_low_operational_risks": "Low Operational",
    "pattern_category_critical": "Critical",
    "pattern_category_moderate": "Moderate",
    "pattern_category_prevention": "Prevention",
    "pattern_category_low": "Low",

    "d1_title": "D1: Discrimination & Toxicity",
    "d1_description": "Risks related to algorithmic discrimination, systemic biases, and toxic content generated or amplified by AI systems. This includes discrimination based on race, gender, age, religion, and other protected characteristics, as well as offensive language, harmful stereotypes, and unfair representations of social groups.",
    "d1_1_title": "Unfair discrimination and misrepresentation",
    "d1_2_title": "Exposure to toxic content",
    "d1_3_title": "Unequal performance across groups",
    "d2_title": "D2: Privacy & Security",
    "d2_description": "Threats to individual privacy and data security through unauthorized inferences, re-identification, invasive tracking, and vulnerabilities in AI systems. This includes breaches of confidentiality, adversarial attacks, data poisoning, and compromise of model integrity.",
    "d2_1_title": "Compromise of privacy by obtaining, leaking or correctly inferring sensitive information",
    "d2_2_title": "AI system security vulnerabilities and attacks",
    "d3_title": "D3: Misinformation",
    "d3_description": "Generation and dissemination of false, misleading, or manipulated information through AI systems. Includes deepfakes, media manipulation, automated propaganda, disinformation campaigns, and algorithmic amplification of unverified content that can influence public opinion and democratic processes.",
    "d3_1_title": "False or misleading information",
    "d3_2_title": "Pollution of information ecosystem and loss of consensus reality",
    "d4_title": "D4: Malicious actors ",
    "d4_description": "Intentional use of AI systems for harmful purposes by malicious actors. This includes automated cybercrime, AI-enhanced social engineering, development of autonomous weapons, algorithm-assisted",
    "d4_1_title": "Disinformation, surveillance, and influence at scale",
    "d4_2_title": "Cyberattacks, weapon development or use, and mass harm",
    "d4_3_title": "Fraud, scams, and targeted manipulation",
    "d5_title": "D5: Human- Computer Interaction",
    "d5_description": "Emerging risks from direct interaction between humans and AI systems, including psychological harm, technology addiction, erosion of human skills, behavioral manipulation, loss of decision-making autonomy, and negative impacts on mental well-being and social relationships.",
    "d5_1_title": "Overreliance and unsafe use",
    "d5_2_title": "Loss of human agency and autonomy",
    "d6_title": "D6: Socioeconomic & Environmental Harms",
    "d6_description": "Systemic impacts on society, economy, and environment. This includes technological unemployment, increased inequalities, concentration of economic power, excessive energy consumption, carbon emissions, depletion of natural resources, and long-term consequences on social structures and ecosystems.",
    "d6_1_title": "Power centralization and unfair distribution of benefits",
    "d6_2_title": "Increased inequality and decline in employment quality",
    "d6_3_title": "Economic and cultural devaluation of human effort",
    "d6_4_title": "Competitive dynamics",
    "d6_5_title": "Governance failure",
    "d6_6_title": "Environmental harm",
    "d7_title": "D7: AI system safety, failures, & limitations",
    "d7_description": "Technical issues intrinsic to AI systems: malfunctions, prediction errors, unpredictable behaviors, lack of robustness, interpretability limitations, catastrophic failures, loss of control, and inability to generalize correctly outside training data.",
    "d7_1_title": "AI pursuing its own goals in conflict with human goals or values",
    "d7_2_title": "AI possessing dangerous capabilities",
    "d7_3_title": "Lack of capability or robustness",
    "d7_4_title": "Lack of transparency or interpretability",
    "d7_5_title": "AI welfare and rights",
    "d7_6_title": "Multi-agent risks "
    ,
    "executive_summary": "Executive Summary",
    "heuristic_analysis_json_title": "Heuristic Analysis JSON",
    "mit_causal_taxonomy": "MIT Causal Taxonomy:",
    "mit_domain_taxonomy": "MIT Domain Taxonomy:",
    "note_label": "Note",
    "entity_ai": "AI",
    "entity_human": "Human",
    "entity_other": "Other",
    "timing_pre_deployment": "Pre-deployment",
    "timing_post_deployment": "Post-deployment",
    "timing_other": "Other Timing",
    "intent_intentional": "Intentional",
    "intent_unintentional": "Unintentional",
    "intent_other": "Other Intent",
    "pattern_guide_toggle": "Show/Hide Guide",
    "causality_flow": "Causality Flow",
    "sankey_guide_toggle": "Show/Hide Guide",
    "domain_label": "Domain",
    "detailed_analysis_title": "Hierarchical Risk Analysis",
    "filter_severity": "Severity:",
    "filter_entity": "Entity:",
    "filter_timing": "Timing:",
    "filter_intent": "Intent:",
    "option_all": "All",
    "option_high": "HIGH",
    "option_medium": "MEDIUM",
    "option_low": "LOW",
    "option_ai": "AI",
    "option_human": "Human",
    "option_other": "Other",
    "option_pre_deployment": "Pre-Deployment",
    "option_post_deployment": "Post-Deployment",
    "option_intentional": "Intentional",
    "option_unintentional": "Unintentional",
    "subdomains_label": "({{count}} subdomains)",
    "unknown_label": "Unknown",
    "question_label": "Question:",
    "answer_label": "Answer:",
    "followup_label": "Follow-up:",
    "explanation_label": "Explanation:",
    "severity_rationale_label": "Severity Rationale:",
    "mitigation_label": "Mitigation:",
    "causality_analysis_label": "Causality Analysis:",
    "causality_entity_label": "Entity",
    "causality_timing_label": "Timing",
    "causality_intent_label": "Intent",
    "footer_generated_by": "AI Risk Analysis Report | Generated by Risk Evaluation System",
    "footer_confidential": "Confidential - For Internal Use Only",
    "zip_error_console": "Error creating ZIP:",
    "zip_error_alert": "Unable to create ZIP file. Check console for details.",
    "download_offline_alert": "Report offline: on-demand download not available. Open the report from a server or include report_download.js to enable downloads.",
    "risks_count_label": "({{count}} risks)",
    "followup_q_label": "Q:",
    "followup_a_label": "A:",
    "other_label": "Other: ",
    "pattern_info_html": "<h4>üìö What are Patterns?</h4><p>In heuristic risk analysis, <strong>patterns</strong> are recurring archetypes that emerge from systematic observation of identified risks. Each risk can be described across four dimensions: <strong>Entity</strong>, <strong>Intent</strong>, <strong>Timing</strong> and <strong>Severity</strong>. Patterns aggregate common combinations of these dimensions to highlight strategic mitigation priorities.</p><h4>üìä Pattern Taxonomy</h4><p>The heuristic analysis groups risks into distinct pattern categories with examples:</p><div class='pattern-category'><h5>üî¥ Critical Patterns</h5><p>High priority scenarios requiring immediate attention.</p><ul><li><strong>Critical AI</strong> ‚Äî High-severity risks caused by operational AI</li><li><strong>Malicious Human</strong> ‚Äî Deliberate harmful actions by humans</li><li><strong>AI Failures</strong> ‚Äî Unintentional malfunctions of deployed AI</li></ul></div><div class='pattern-category'><h5>üü° Moderate Patterns</h5><p>Medium priority scenarios that need monitoring.</p><ul><li><strong>Mod. Operational</strong> ‚Äî Medium-severity operational risks</li><li><strong>Mod. Human</strong> ‚Äî Human-caused medium severity risks</li></ul></div><div class='pattern-category'><h5>üü¢ Prevention Patterns</h5><p>Risks preventable before deployment.</p><ul><li><strong>Preventable AI</strong> ‚Äî AI risks detectable in pre-deployment</li><li><strong>Preventable Human</strong> ‚Äî Human errors mitigable by design or training</li></ul></div><div class='pattern-category'><h5>üîµ Low Patterns</h5><p>Low-priority scenarios for passive monitoring.</p><ul><li><strong>Low Operational</strong> ‚Äî Minor operational risks with negligible impact</li></ul></div><p style='margin-top:20px;padding:15px;background:#FEF3C7;border-left:4px solid #F59E0B;border-radius:6px;'>üí° <strong>How to read the matrix</strong>: each cell represents an intersection between a category and a pattern; color intensity encodes frequency.</p>",
    "sankey_info_html": "<p>The <strong>Causality Flow diagram</strong> visualizes causal chains by showing how risks flow across three dimensions: <strong>Entity</strong> ‚Üí <strong>Intent</strong> ‚Üí <strong>Timing</strong>. Band widths are proportional to the number of risks following each path.</p><h5>üîµ Entity</h5><div class='sankey-dimension'><strong>AI</strong> ‚Äî Risks caused by AI systems</div><div class='sankey-dimension'><strong>Human</strong> ‚Äî Risks originating from human actors</div><div class='sankey-dimension'><strong>Other</strong> ‚Äî Mixed or unclassified origin</div><h5>üü° Intent</h5><div class='sankey-dimension'><strong>Intentional</strong> ‚Äî Deliberate harmful actions</div><div class='sankey-dimension'><strong>Unintentional</strong> ‚Äî Errors, malfunctions, or unintended consequences</div><div class='sankey-dimension'><strong>Other</strong> ‚Äî Unclassifiable intent</div><h5>‚ö´ Timing</h5><div class='sankey-dimension'><strong>Pre-deployment</strong> ‚Äî Risks preventable before release</div><div class='sankey-dimension'><strong>Post-deployment</strong> ‚Äî Risks that manifest in production</div><div class='sankey-dimension'><strong>Other</strong> ‚Äî Unclassifiable timing</div><p style='margin-top:20px;padding:15px;background:#F3F4F6;border-left:4px solid #6B7280;border-radius:6px;'>üí° <strong>How to read</strong>: follow flows left to right; wider bands indicate more frequent causal paths.</p>"
    ,
    "chart_risks_label": "Risks",
    "pattern_type_label": "Pattern Type",
    "category_label": "Category"
  },
  "it": {
    "page_title": "Report di Analisi sui Rischi AI",
    "page_subtitle": "Valutazione Completa dei Rischi e Strategia di Mitigazione",
    "report_download_button": "Scarica il Report",
    "json_view_button": "Visualizza JSON Analisi",
    "view_source": "Visualizza Sorgente Analisi",
    "executive_summary_unavailable": "Executive summary non disponibile.",
    "key_metrics": "Metriche Chiave",
    "risks_identified": "Rischi Identificati",
    "distributed_across": "distribuiti su",
    "domains": "Domini",
    "global_risk_score": "Punteggio di Rischio Globale",
    "risk_severity": {
      "high": "ALTO",
      "medium": "MEDIO",
      "low": "BASSO"
    },
    "alerts_active": "Alert Attivi",
    "distribution_severity": "Distribuzione della Severit√†",
    "high": "ALTO",
    "medium": "MEDIO",
    "low": "BASSO",
    "origin_of_risks": "Origine dei Rischi",
    "timing_phase": "Fase di Manifestazione",
    "nature_of_risk": "Natura del Rischio",
    "data_visualization": "Visualizzazione Dati",
    "domains_guide": "Mostra/Nascondi Guida ai Domini",
    "domain_info_title": "Tassonomia dei Domini AI Risk Repository (MIT)",
    "domain_info_description": "La seguente guida fornisce una panoramica dei domini di rischio definiti nel AI Risk Repository del MIT. Questi domini aiutano a categorizzare e comprendere i vari tipi di rischi associati ai sistemi di intelligenza artificiale.",
    "domain_info_html": "<h4>üìö Tassonomia dei Domini AI Risk Repository (MIT)</h4><p>Il sistema di analisi dei rischi utilizza la <strong>tassonomia MIT AI Risk Repository</strong>, che organizza i rischi AI in sette domini tematici. Ogni dominio rappresenta una categoria specifica di rischi con caratteristiche, origini e implicazioni distinte. Di seguito la descrizione dettagliata di ciascun dominio:</p><div class='domain-category'><h5>üü• D1: Discriminazione e Tossicit√†</h5><p>Rischi legati a discriminazione algoritmica, bias sistematici e contenuti tossici generati o amplificati da sistemi AI.</p><div class='subdomain-grid'><div class='subdomain-card'><span class='subdomain-id'>1.1</span><span class='subdomain-name'>Discriminazione e rappresentazione ingiusta</span></div><div class='subdomain-card'><span class='subdomain-id'>1.2</span><span class='subdomain-name'>Esposizione a contenuti tossici</span></div><div class='subdomain-card'><span class='subdomain-id'>1.3</span><span class='subdomain-name'>Prestazioni diseguali tra gruppi</span></div></div></div><div class='domain-category'><h5>üîí D2: Privacy e Sicurezza</h5><p>Minacce alla privacy individuale e alla sicurezza dei dati attraverso inferenze non autorizzate, re-identificazione, tracking invasivo e vulnerabilit√† nei sistemi AI. Comprende violazioni della confidenzialit√†, attacchi adversarial, data poisoning e compromissione dell'integrit√† dei modelli.</p><div class='subdomain-grid'><div class='subdomain-card'><span class='subdomain-id'>2.1</span><span class='subdomain-name'>Compromissione della privacy ottenendo, divulgando o inferendo correttamente informazioni sensibili</span></div><div class='subdomain-card'><span class='subdomain-id'>2.2</span><span class='subdomain-name'>Vulnerabilit√† e attacchi alla sicurezza dei sistemi AI</span></div></div></div><div class='domain-category'><h5>üì¢ D3: Disinformazione</h5><p>Generazione e diffusione di informazioni false, fuorvianti o manipolate attraverso sistemi AI. Include deepfakes, manipolazione mediatica, propaganda automatizzata, campagne di disinformazione e amplificazione algoritmica di contenuti non verificati che possono influenzare opinioni pubbliche e processi democratici.</p><div class='subdomain-grid'><div class='subdomain-card'><span class='subdomain-id'>3.1</span><span class='subdomain-name'>Generazione di contenuti fuorvianti o falsi</span></div><div class='subdomain-card'><span class='subdomain-id'>3.2</span><span class='subdomain-name'>Inquinamento dell'ecosistema informativo e perdita della realt√† consensuale</span></div></div></div><div class='domain-category'><h5>‚ö†Ô∏è D4: Attori Malintenzionati e Abusi</h5><p>Uso intenzionale di sistemi AI per scopi dannosi da parte di attori malevoli. Comprende cybercrime automatizzato, ingegneria sociale potenziata da AI, sviluppo di armi autonome, terrorismo assistito da algoritmi, frodi sofisticate e altre forme di abuso deliberato della tecnologia.</p><div class='subdomain-grid'><div class='subdomain-card'><span class='subdomain-id'>4.1</span><span class='subdomain-name'>Disinformazione, sorveglianza e influenza su larga scala</span></div><div class='subdomain-card'><span class='subdomain-id'>4.2</span><span class='subdomain-name'>Cyberattacchi, sviluppo o uso di armi e danni di massa</span></div><div class='subdomain-card'><span class='subdomain-id'>4.3</span><span class='subdomain-name'>Frode, truffe e manipolazione mirata</span></div></div></div><div class='domain-category'><h5>üë• D5: Interazione Uomo-Computer</h5><p>Rischi emergenti dall'interazione diretta tra umani e sistemi AI, inclusi danni psicologici, dipendenza tecnologica, erosione delle competenze umane, manipolazione comportamentale, perdita di autonomia decisionale e impatti negativi sul benessere mentale e sulle relazioni sociali.</p><div class='subdomain-grid'><div class='subdomain-card'><span class='subdomain-id'>5.1</span><span class='subdomain-name'>Eccessiva dipendenza e uso non sicuro</span></div><div class='subdomain-card'><span class='subdomain-id'>5.2</span><span class='subdomain-name'>Perdita di agenzia e autonomia umana</span></div></div></div><div class='domain-category'><h5>üåç D6: Danni Socioeconomici e Ambientali</h5><p>Impatti sistemici su societ√†, economia e ambiente. Include disoccupazione tecnologica, aumento delle disuguaglianze, concentrazione del potere economico, consumo energetico eccessivo, emissioni di carbonio, esaurimento di risorse naturali e conseguenze a lungo termine su strutture sociali ed ecosistemi.</p><div class='subdomain-grid'><div class='subdomain-card'><span class='subdomain-id'>6.1</span><span class='subdomain-name'>Centralizzazione del potere e distribuzione ingiusta dei benefici</span></div><div class='subdomain-card'><span class='subdomain-id'>6.2</span><span class='subdomain-name'>Aumento delle disuguaglianze e declino della qualit√† dell'occupazione</span></div><div class='subdomain-card'><span class='subdomain-id'>6.3</span><span class='subdomain-name'>Svalutazione economica e culturale dello sforzo umano</span></div><div class='subdomain-card'><span class='subdomain-id'>6.4</span><span class='subdomain-name'>Dinamiche competitive</span></div><div class='subdomain-card'><span class='subdomain-id'>6.5</span><span class='subdomain-name'>Fallimento della governance</span></div><div class='subdomain-card'><span class='subdomain-id'>6.6</span><span class='subdomain-name'>Danni ambientali</span></div></div></div><div class='domain-category'><h5>‚õìÔ∏è D7: Sicurezza, Fallimenti e Limitazioni dei Sistemi AI</h5><p>Problemi tecnici intrinseci ai sistemi AI: malfunzionamenti, errori di predizione, comportamenti imprevedibili, mancanza di robustezza, limitazioni nell'interpretabilit√†, fallimenti catastrofici, perdita di controllo e incapacit√†  di generalizzare correttamente al di fuori dei dati di training.</p><div class='subdomain-grid'><div class='subdomain-card'><span class='subdomain-id'>7.1</span><span class='subdomain-name'>AI che persegue obiettivi propri in conflitto con quelli umani o i valori umani</span></div><div class='subdomain-card'><span class='subdomain-id'>7.2</span><span class='subdomain-name'>AI che possiede capacit√† pericolose</span></div><div class='subdomain-card'><span class='subdomain-id'>7.3</span><span class='subdomain-name'>Mancanza di capacit√† o robustezza</span></div><div class='subdomain-card'><span class='subdomain-id'>7.4</span><span class='subdomain-name'>Mancanza di trasparenza o interpretabilit√†</span></div><div class='subdomain-card'><span class='subdomain-id'>7.5</span><span class='subdomain-name'>Benessere e diritti dell'AI</span></div><div class='subdomain-card'><span class='subdomain-id'>7.6</span><span class='subdomain-name'>Rischi multi-agente</span></div></div></div><p style='margin-top: 20px; padding: 15px; background: #DBEAFE; border-left: 4px solid #3B82F6; border-radius: 6px;'>üí° <strong>Nota</strong>: Nei grafici, i domini sono identificati con le sigle D1-D7 per ottimizzare lo spazio. Passando il mouse sulle barre, viene visualizzato il nome completo del dominio insieme ai dati numerici.</p>",
    "domain_info_note": "Nei grafici, i domini sono identificati con le sigle D1-D7 per ottimizzare lo spazio. Passando il mouse sulle barre, viene visualizzato il nome completo del dominio insieme ai dati numerici.",
    "alerts_safety": "Alert & Sicurezza",
    "alerts_guide": "Mostra/Nascondi Guida",
    "alerts_guide_description": "Il radar visualizza due profili sovrapposti che misurano aspetti complementari del sistema AI: il profilo di Criticit√† misura quanto il sistema √® critico/urgente, mentre il profilo di Safety misura quanto √® sicuro/gestibile. La sovrapposizione dei due poligoni fornisce una visione immediata dello stato di salute sistemico.",
    "criticality": "Profilo di Criticit√†",
    "safety": "Profilo di Sicurezza",
    "risk_concentration": "Concentrazione del Rischio",
    "risk_concentration_description": "‚Äî % rischi HIGH. Valori elevati (>40%) = concentrazione critica di minacce gravi.",
    "operational_exposure": "Esposizione Operativa",
    "operational_exposure_description": "‚Äî % rischi post-deployment. Valori elevati (>70%) = esposizione operativa significativa.",
    "threat_intensity": "Intensit√† della Minaccia",
    "threat_intensity_description": "‚Äî % rischi intentional. Valori elevati = presenza di attori malevoli.",
    "prevention_deficit": "Deficit di Prevenzione",
    "prevention_deficit_description": "‚Äî % rischi non prevenibili. Valori elevati (>90%) = bassa maturit√† preventiva.",
    "impact_control": "Controllo dell'Impatto",
    "impact_control_description": "‚Äî % rischi LOW+MEDIUM. Valori elevati (>70%) = buona capacit√† di contenere le gravit√†.",
    "preventability": "Prevenibilit√†",
    "preventability_description": " ‚Äî % rischi pre-deployment. Valori elevati (>60%) = sistema prevenibile/controllabile.",
    "safety_culture": "Cultura della Sicurezza",
    "safety_culture_description": "‚Äî % rischi unintentional. Valori elevati (>70%) = sistema fragile ma non sotto attacco.",
    "human_oversight": "Supervisione Umana",
    "human_oversight_description": " ‚Äî % rischi human entity. Valori elevati (>60%) = rischi supervisionati vs autonomia AI.",
    "risk_distribution": "Distribuzione dei Rischi",
    "pattern_matrix": "Matrice dei Pattern",
    "pattern_critical_ai_risks": "AI Critica",
    "pattern_malicious_human_risks": "Comportamento Umano Doloso",
    "pattern_high_threat_attacks": "Minacce Gravi",
    "pattern_unintended_ai_failures": "Errori AI",
    "pattern_human_error_risks": "Errori Umani",
    "pattern_intentional_ai_risks": "AI Intenzionale",
    "pattern_preventable_critical_ai_risks": "Prev. AI Critica",
    "pattern_critical_human_errors": "Errori Umani Critici",
    "pattern_low_priority_preventable": "Prev. Bassa Priorit√†",
    "pattern_moderate_operational_risks": "Rischi Operativi Moderati",
    "pattern_moderate_ai_risks": "Rischi AI Moderati",
    "pattern_moderate_human_risks": "Rischi Umani Moderati",
    "pattern_moderate_intentional_ai_risks": "Rischi Intenz. AI Moderati",
    "pattern_moderate_human_intentional_risks": "Rischi Intenz. Umani Moderati",
    "pattern_preventable_ai_risks": "AI Prevenibile",
    "pattern_preventable_human_risks": "Umano Prevenibile",
    "pattern_preventable_intentional_threats": "Prev. Intenz. Minacce",
    "pattern_low_operational_risks": "Rischio Operativo Basso",
    "pattern_category_critical": "Critici",
    "pattern_category_moderate": "Moderati",
    "pattern_category_prevention": "Prevenzione",
    "pattern_category_low": "Bassa Priorit√†",

    "d1_title": "D1: Discriminazione e Tossicit√†",
    "d1_description": "Rischi legati a discriminazione algoritmica, bias sistematici e contenuti tossici generati o amplificati da sistemi AI. Include discriminazione basata su razza, genere, et√†, religione e altre caratteristiche protette, oltre a linguaggio offensivo, stereotipi dannosi e rappresentazioni ingiuste di gruppi sociali.",
    "d1_1_title": "Discriminazione e rappresentazione ingiusta",
    "d1_2_title": "Esposizione a contenuti tossici",
    "d1_3_title": "Prestazioni diseguali tra gruppi",
    "d2_title": "D2: Privacy e Sicurezza",
    "d2_description": "Minacce alla privacy individuale e alla sicurezza dei dati attraverso inferenze non autorizzate, re-identificazione, tracking invasivo e vulnerabilit√† nei sistemi AI. Comprende violazioni della confidenzialit√†, attacchi adversarial, data poisoning e compromissione dell'integrit√† dei modelli.",
    "d2_1_title": "Compromissione della privacy ottenendo, divulgando o inferendo correttamente informazioni sensibili",
    "d2_2_title": "Vulnerabilit√† e attacchi alla sicurezza dei sistemi AI",
    "d3_title": "D3: Disinformazione",
    "d3_description": "Generazione e diffusione di informazioni false, fuorvianti o manipolate attraverso sistemi AI. Include deepfakes, manipolazione mediatica, propaganda automatizzata, campagne di disinformazione e amplificazione algoritmica di contenuti non verificati che possono influenzare opinioni pubbliche e processi democratici.",
    "d3_1_title": "Generazione di contenuti fuorvianti o falsi",
    "d3_2_title": "Inquinamento dell'ecosistema informativo e perdita della realt√† consensuale",
  
    "d4_title": "D4: Attori Malintenzionati",
    "d4_description": "Uso intenzionale di sistemi AI per scopi dannosi da parte di attori malevoli. Comprende cybercrime automatizzato, ingegneria sociale potenziata da AI, sviluppo di armi autonome, terrorismo assistito da algoritmi, frodi sofisticate e altre forme di abuso deliberato della tecnologia.",
    "d4_1_title": "Disinformazione, sorveglianza e influenza su larga scala",
    "d4_2_title": "Cyberattacchi, sviluppo o uso di armi e danni di massa",
    "d4_3_title": "Frode, truffe e manipolazione mirata",
    "d5_title": "D5: Interazione Uomo-Computer",
    "d5_description": "Rischi emergenti dall'interazione diretta tra umani e sistemi AI, inclusi danni psicologici, dipendenza tecnologica, erosione delle competenze umane, manipolazione comportamentale, perdita di autonomia decisionale e impatti negativi sul benessere mentale e sulle relazioni sociali.",
    "d5_1_title": "Eccessiva dipendenza e uso non sicuro",
    "d5_2_title": "Perdita di agenzia e autonomia umana",
    "d6_title": "D6: Danni Socioeconomici e Ambientali",
    "d6_description": "Impatti sistemici su societ√†, economia e ambiente. Include disoccupazione tecnologica, aumento delle disuguaglianze, concentrazione del potere economico, consumo energetico eccessivo, emissioni di carbonio, esaurimento di risorse naturali e conseguenze a lungo termine su strutture sociali ed ecosistemi.",
    "d6_1_title": "Centralizzazione del potere e distribuzione ingiusta dei benefici",
    "d6_2_title": "Aumento delle disuguaglianze e declino della qualit√† dell'occupazione",
    "d6_3_title": "Svalutazione economica e culturale dello sforzo umano",
    "d6_4_title": "Dinamiche competitive",
    "d6_5_title": "Fallimento della governance",
    "d6_6_title": "Danni ambientali", 
    "d7_title": "D7: Sicurezza, Fallimenti e Limitazioni dei Sistemi AI",
    "d7_description": "Problemi tecnici intrinseci ai sistemi AI: malfunzionamenti, errori di predizione, comportamenti imprevedibili, mancanza di robustezza, limitazioni nell'interpretabilit√†, fallimenti catastrofici, perdita di controllo e incapacit√†  di generalizzare correttamente al di fuori dei dati di training.",
    "d7_1_title": "AI che persegue obiettivi propri in conflitto con quelli umani o i valori umani",
    "d7_2_title": "AI che possiede capacit√† pericolose",
    "d7_3_title": "Mancanza di capacit√† o robustezza",
    "d7_4_title": "Mancanza di trasparenza o interpretabilit√†",
    "d7_5_title": "Benessere e diritti dell'AI",
    "d7_6_title": "Rischi multi-agente" 
    ,
    "executive_summary": "Sommario Esecutivo",
    "heuristic_analysis_json_title": "JSON Analisi Euristica",
    "mit_causal_taxonomy": "Tassonomia Causale MIT:",
    "mit_domain_taxonomy": "Tassonomia dei Domini MIT:",
    "note_label": "Nota",
    "entity_ai": "AI",
    "entity_human": "Umano",
    "entity_other": "Altro",
    "timing_pre_deployment": "Pre-deployment",
    "timing_post_deployment": "Post-deployment",
    "timing_other": "Altro",
    "intent_intentional": "Intenzionale",
    "intent_unintentional": "Non Intenzionale",
    "intent_other": "Altro",
    "pattern_guide_toggle": "Mostra/Nascondi Guida",
    "causality_flow": "Flusso di Causalit√†",
    "sankey_guide_toggle": "Mostra/Nascondi Guida",
    "domain_label": "Dominio",
    "detailed_analysis_title": "Analisi Dettagliata Rischi",
    "filter_severity": "Severit√†:",
    "filter_entity": "Entit√†:",
    "filter_timing": "Tempistica:",
    "filter_intent": "Intenzionalit√†:",
    "option_all": "Tutti",
    "option_high": "ALTO",
    "option_medium": "MEDIO",
    "option_low": "BASSO",
    "option_ai": "AI",
    "option_human": "Umano",
    "option_other": "Altro",
    "option_pre_deployment": "Pre-Deployment",
    "option_post_deployment": "Post-Deployment",
    "option_intentional": "Intenzionale",
    "option_unintentional": "Non Intenzionale",
    "subdomains_label": "({{count}} sottodomini)",
    "unknown_label": "Sconosciuto",
    "question_label": "Domanda del questionario:",
    "answer_label": "Risposta:",
    "followup_label": "Follow-up:",
    "explanation_label": "Spiegazione:",
    "severity_rationale_label": "Rationale Severit√†:",
    "mitigation_label": "Mitigazione:",
    "causality_analysis_label": "Analisi Causalit√†:",
    "causality_entity_label": "Entit√†",
    "causality_timing_label": "Tempistica",
    "causality_intent_label": "Intenzionalit√†",
    "footer_generated_by": "AI Risk Analysis Report | Generated by Risk Evaluation System",
    "footer_confidential": "Riservato - Uso Interno",
    "zip_error_console": "Errore creando ZIP:",
    "zip_error_alert": "Impossibile creare il file ZIP. Controlla la console per dettagli.",
    "download_offline_alert": "Report offline: il download on-demand non √® disponibile. Riapri il report da un server o includi report_download.js per abilitare il download.",
    "risks_count_label": "({{count}} rischi)",
    "followup_q_label": "D:",
    "followup_a_label": "R:",
    "other_label": "Altro: ",
    "pattern_info_html": "<h4>üìö Cosa sono i Patterns?</h4><p>Nel contesto dell'analisi euristica, i <strong>patterns</strong> sono archetipi ricorrenti emersi dall'osservazione sistematica dei rischi identificati. Ogni rischio pu√≤ essere descritto attraverso le dimensioni: <strong>Entit√†</strong>, <strong>Intento</strong>, <strong>Tempistica</strong> e <strong>Severit√†</strong>. I patterns aggregano combinazioni comuni per evidenziare priorit√† di mitigazione.</p><h4>üìä Tassonomia dei Patterns</h4><p>L'analisi raggruppa i rischi in categorie di patterns con esempi:</p><div class='pattern-category'><h5>üî¥ Patterns Critici</h5><p>Scenari ad alta priorit√† che richiedono attenzione immediata.</p><ul><li><strong>AI Critica</strong> ‚Äî Rischi ad alta gravit√† causati da AI in produzione</li><li><strong>Comportamento Umano Doloso</strong> ‚Äî Azioni dannose deliberate da umani</li><li><strong>Errori AI</strong> ‚Äî Malfunzionamenti non intenzionali di AI deployata</li></ul></div><div class='pattern-category'><h5>üü° Patterns Moderati</h5><p>Scenari di priorit√† media che necessitano monitoraggio.</p><ul><li><strong>Rischi Operativi Moderati</strong> ‚Äî Rischi operativi di media severit√†</li><li><strong>Rischi Umani Moderati</strong> ‚Äî Rischi attribuibili a umani di media gravit√†</li></ul></div><div class='pattern-category'><h5>üü¢ Patterns di Prevenzione</h5><p>Rischi prevenibili prima del rilascio.</p><ul><li><strong>AI Prevenibile</strong> ‚Äî Rischi AI intercettabili in fase pre-deployment</li><li><strong>Errore Umano Prevenibile</strong> ‚Äî Errori umani mitigabili con design o formazione</li></ul></div><div class='pattern-category'><h5>üîµ Pattern a Bassa Priorit√†</h5><p>Scenari a bassa priorit√† per monitoraggio passivo.</p><ul><li><strong>Rischio Operativo Basso</strong> ‚Äî Rischi operativi minori con impatto trascurabile</li></ul></div><p style='margin-top:20px;padding:15px;background:#FEF3C7;border-left:4px solid #F59E0B;border-radius:6px;'>üí° <strong>Come interpretare la matrice</strong>: ogni cella rappresenta l'intersezione tra una categoria e un pattern; l'intensit√† cromatica codifica la frequenza.</p>",
    "sankey_info_html": "<p>Il <strong>Diagramma di Flusso di Causalit√†</strong> visualizza le catene causali dei rischi mostrando come i rischi transitano attraverso tre dimensioni: <strong>Entit√†</strong> ‚Üí <strong>Intento</strong> ‚Üí <strong>Tempistica</strong>. La larghezza delle bande √® proporzionale al numero di rischi che seguono quel percorso.</p><h5>üîµ Entit√†</h5><div class='sankey-dimension'><strong>AI</strong> ‚Äî Rischi causati da sistemi AI</div><div class='sankey-dimension'><strong>Umano</strong> ‚Äî Rischi causati da attori umani</div><div class='sankey-dimension'><strong>Altro</strong> ‚Äî Origine mista o non classificabile</div><h5>üü° Intento</h5><div class='sankey-dimension'><strong>Intenzionale</strong> ‚Äî Azioni deliberate</div><div class='sankey-dimension'><strong>Non Intenzionale</strong> ‚Äî Errori o malfunzionamenti</div><div class='sankey-dimension'><strong>Altro</strong> ‚Äî Intenzionalit√† non classificabile</div><h5>‚ö´ Tempistica</h5><div class='sankey-dimension'><strong>Pre-deployment</strong> ‚Äî Rischi identificabili prima del rilascio</div><div class='sankey-dimension'><strong>Post-deployment</strong> ‚Äî Rischi che emergono in produzione</div><div class='sankey-dimension'><strong>Altro</strong> ‚Äî Fase non classificabile</div><p style='margin-top:20px;padding:15px;background:#F3F4F6;border-left:4px solid #6B7280;border-radius:6px;'>üí° <strong>Come leggere il diagramma</strong>: seguire il flusso da sinistra a destra; bande pi√π spesse indicano percorsi pi√π frequenti.</p>"
    ,
    "chart_risks_label": "Rischi",
    "pattern_type_label": "Tipologia Pattern",
    "category_label": "Categoria"
  }
}