from __future__ import annotations

import argparse
import json
from datetime import datetime
from typing import Any, Dict, List

from jinja2 import Template
from langchain.agents import create_agent
from langchain_core.messages import AIMessage, BaseMessage
from langchain_core.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate

from . import prompts as qprompt
from utils.utils import get_llm_instance, create_logger

DEFAULT_PROFILE_TEMPS: Dict[str, float] = {
    "expert": 0.2,
    "intermediate": 0.5,
    "beginner": 0.9,
}

_logger = create_logger("questionnaire_generator_agent")


def load_questions(path: str = "files/") -> List[Dict[str, Any]]:
    """
    Load and return the list of questions from the JSON file.

    Args:
        path (str): The file path to the JSON file containing questions.

    Returns:
        List[Dict[str, Any]]: A list of questions loaded from the file.

    Raises:
        FileNotFoundError: If the file does not exist.
        json.JSONDecodeError: If the file is not a valid JSON.
    """
    try:
        with open(path, "r", encoding="utf-8") as fh:
            data = json.load(fh)

        questions = data.get("questions", [])
        if not isinstance(questions, list):
            _logger.error("The 'questions' field is not a list in the file %s", path)
            raise ValueError("'questions' must be a list in the JSON file")

        _logger.info(
            "Questions file loaded successfully: number of questions=%d, path=%s",
            len(questions),
            path,
        )

        return questions

    except FileNotFoundError:
        _logger.error("Questions file not found: %s", path)
        raise
    except json.JSONDecodeError as e:
        _logger.error("JSON decoding error in %s: %s", path, e)
        raise


def configure_model_context_template(
    profile: str, language: str
) -> SystemMessagePromptTemplate:
    """
    Configure the initial model context using Jinja manually.

    Args:
        profile (str): The simulated profile (e.g., "Expert", "Intermediate", "Beginner").
        language (str): The language for the questionnaire.

    Returns:
        str: The rendered system prompt.
    """
    template = Template(qprompt.QUESTIONNAIRE_SYSTEM_PROMPT)
    rendered_prompt = template.render(profile=profile, language=language)
    _logger.info("Initial context configured successfully.")
    return rendered_prompt


def build_user_prompt(questions: List[Dict[str, Any]], language: str) -> str:
    """
    Build the user prompt for the questionnaire using Jinja manually.

    Args:
        questions (List[Dict[str, Any]]): The list of questions for the questionnaire.
        language (str): The language for the questionnaire.

    Returns:
        str: The rendered user prompt.
    """
    template = Template(qprompt.QUESTIONNAIRE_USER_PROMPT)
    questions_json = json.dumps(questions, ensure_ascii=False)
    rendered_prompt = template.render(questions_json=questions_json, language=language)
    _logger.info("User prompt built successfully.")
    return rendered_prompt


def build_chat_messages(
    questions: List[Dict[str, Any]],
    profile: str,
    language: str,
) -> List[BaseMessage]:
    """
    Builds chat messages (System + Human) using ChatPromptTemplate and Jinja2.

    Args:
        questions (List[Dict[str, Any]]): The list of questions for the questionnaire.
        profile (str): The simulated profile (e.g., "Expert", "Intermediate", "Beginner").
        language (str): The language for the questionnaire.

    Returns:
        List[BaseMessage]: The list of chat messages ready for the agent.
    """
    system_tpl = configure_model_context_template(profile, language)
    human_tpl = build_user_prompt(questions, language)

    chat_prompt = ChatPromptTemplate.from_messages(
        [system_tpl, human_tpl],
        template_format="jinja2",
    )

    questions_json = json.dumps(questions, ensure_ascii=False)

    formatted = chat_prompt.format_prompt(
        profile=profile,
        language=language,
        questions_json=questions_json,
    )
    messages = formatted.to_messages()
    _logger.info("Input prompt built successfully.")
    return messages


def generate_responses(
    questions: List[Dict[str, Any]], profile: str, language: str
) -> Any:
    """
    Builds the LLM with the temperature for the profile, creates the agent, invokes it, and returns the response.

    Args:
        questions (List[Dict[str, Any]]): The list of questions for the questionnaire.
        profile (str): The simulated profile (e.g., "Expert", "Intermediate", "Beginner").
        language (str): The language for the questionnaire.

    Returns:
        Any: The complete response generated by the agent.
    """
    temp = DEFAULT_PROFILE_TEMPS.get(profile, 0.5)
    llm = get_llm_instance(t=temp)

    agent = create_agent(model=llm)
    messages = build_chat_messages(questions, profile, language)

    # Invoke and directly return the agent's response

    response = agent.invoke({"messages": messages})
    _logger.info("Questionnaire generated successfully by the agent.")

    return response


def save_responses_with_metadata(response, profile, language, output_path, run_id=None):
    """
    Saves the generated responses to a JSON file, including metadata.

    Args:
        response (dict): The complete response generated by the agent.
        profile (str): The simulated profile (e.g., "Expert", "Intermediate", "Beginner").
        output_path (str): The file path where the responses will be saved.

    Returns:
        Tuple[str, str]: The run_id and the file name where responses are saved.
    """
    try:
        # Filter all messages of type AIMessage
        ai_messages = [m for m in response["messages"] if isinstance(m, AIMessage)]

        # Combine all responses into a single dictionary
        all_responses = {}
        for msg in ai_messages:
            content = msg.content
            # Clean the content: remove "```json" delimiters if present
            if content.startswith("```json"):
                content = content[7:]
            if content.endswith("```"):
                content = content[:-3]

            # Convert the JSON content to a dictionary and update the overall responses
            parsed_content = json.loads(content)
            all_responses.update(parsed_content)

        # Handle run_id: if not provided, generate a new one
        import uuid

        if run_id is None:
            run_id = uuid.uuid4().hex

        # Generate the file name with run_id
        file_name = f"files/answers/answers_{run_id}.json"

        # Prepare the data to save with metadata
        data_to_save = {
            "metadata": {
                "profile": profile,
                "language": language,
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "run_id": run_id,
            },
            "responses": all_responses,
        }

        # Save the data to the file
        with open(file_name, "w", encoding="utf-8") as f:
            json.dump(data_to_save, f, ensure_ascii=False, indent=4)

        _logger.info("Responses saved successfully: %s", file_name)
        return run_id, file_name

    except Exception as e:
        _logger.error("Error saving responses to %s: %s", output_path, e)
        raise


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Generate responses for the AI questionnaire."
    )
    parser.add_argument(
        "questions_filename",
        type=str,
        help="Name of the questions file (in files/) containing the questionnaire and metadata (must include 'language' in metadata).",
    )
    parser.add_argument(
        "--profile",
        type=str,
        choices=["expert", "intermediate", "beginner"],
        required=True,
        help="Specify the user profile for the questionnaire.",
    )
    args = parser.parse_args()

    try:
        input_path = f"files/{args.questions_filename}"
        with open(input_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        metadata = data.get("metadata", {})
        language = metadata.get("language")
        if not language:
            raise ValueError("Missing 'language' in metadata of input file.")
        questions = load_questions(input_path)
        profile = args.profile
        response = generate_responses(questions, profile, language)

        # Save responses and get run_id and file_name
        run_id, file_name = save_responses_with_metadata(
            response, profile, language, "files/answers/"
        )
        _logger.info(f"Responses saved in {file_name} with run_id: {run_id}")

    except Exception as e:
        _logger.error("Error during response generation: %s", e)
